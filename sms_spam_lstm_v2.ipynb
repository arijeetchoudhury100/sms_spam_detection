{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/sms-spam-collection-dataset/spam.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Load the dataset and get rid of useless columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the data\ndata = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\ndata.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"     v1                                                 v2 Unnamed: 2  \\\n0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n1   ham                      Ok lar... Joking wif u oni...        NaN   \n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3   ham  U dun say so early hor... U c already then say...        NaN   \n4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n\n  Unnamed: 3 Unnamed: 4  \n0        NaN        NaN  \n1        NaN        NaN  \n2        NaN        NaN  \n3        NaN        NaN  \n4        NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get rid of useless columns\ndata = data[['v1','v2']]\ndata.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"     v1                                                 v2\n0   ham  Go until jurong point, crazy.. Available only ...\n1   ham                      Ok lar... Joking wif u oni...\n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3   ham  U dun say so early hor... U c already then say...\n4   ham  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Let us check the distribution of classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.v1.value_counts())\nsns.countplot(data.v1)","execution_count":38,"outputs":[{"output_type":"stream","text":"0    4825\n1     747\nName: v1, dtype: int64\n","name":"stdout"},{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fdc29411510>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"#### The dataset in imbalanced. Accuracy will not be a good metric. We need to evaluate the model using precision/recall, F1-score, cohen-kappa score etc. We might need to perform undersampling/oversampling. We shall look at it later. We change 'ham' to label 0, 'spam' to label 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_label(value):\n    return 0 if value == 'ham' else 1\ndata['v1'] = data['v1'].apply(set_label)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   v1                                                 v2\n0   0  Go until jurong point, crazy.. Available only ...\n1   0                      Ok lar... Joking wif u oni...\n2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n3   0  U dun say so early hor... U c already then say...\n4   0  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning:\n#### 1. remove urls\n#### 2. remove emails\n#### 3. remove tags\n#### 4. remove punctuations\n#### 5. remove stopwords\n#### 6. lemmatize/stem"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(text): \n    translator = str.maketrans('', '', string.punctuation) \n    return text.translate(translator)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_tags(text):\n  return re.sub('<.*?>',\" \",text)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numbers(text):\n  return re.sub('[0-9]+','number',text)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n  return re.sub('https?:\\S+','link',text)\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emails(text):\n    return re.sub('\\S+@\\S+','email',text)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(remove_urls)\ndata['v2'] = data['v2'].apply(remove_tags)\ndata['v2'] = data['v2'].apply(remove_emails)\ndata['v2'] = data['v2'].apply(remove_punctuation)\ndata['v2'] = data['v2'].apply(remove_numbers)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(lambda word : word.lower())","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"   v1                                                 v2\n0   0  go until jurong point crazy available only in ...\n1   0                            ok lar joking wif u oni\n2   1  free entry in number a wkly comp to win fa cup...\n3   0        u dun say so early hor u c already then say\n4   0  nah i dont think he goes to usf he lives aroun...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>go until jurong point crazy available only in ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>ok lar joking wif u oni</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>free entry in number a wkly comp to win fa cup...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>u dun say so early hor u c already then say</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>nah i dont think he goes to usf he lives aroun...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstops = stopwords.words('english')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    cleaned = []\n    for word in text.split():\n        if word not in stops:\n            cleaned.append(word)\n    return \" \".join(cleaned)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(remove_stopwords)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n#nltk.download('wordnet')\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_words(text):\n  lemmas = []\n  for word in text.split():\n    lemmas.append(lemmatizer.lemmatize(word))\n  return \" \".join(lemmas)","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatize and shuffle the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(lemmatize_words)\ndata = data.sample(frac=1).reset_index(drop=True)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['v2'].values\ny = data['v1'].values","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)\nprint('No. of training samples:',len(X_train))\nprint('No. of testing samples:',len(X_test))","execution_count":23,"outputs":[{"output_type":"stream","text":"No. of training samples: 3900\nNo. of testing samples: 1672\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize the data into sequence of tokens. Then pad/truncate the data so that every sequence is of same length. "},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer() \ntokenizer.fit_on_texts(X_train)\nword_to_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(X_train)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_to_index)\nmax_length = 50\nembedding_dim = 100\npadded_sequences = pad_sequences(sequences,maxlen=max_length,padding='post',truncating='post')","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(X_test)\npadded_test_sequences = pad_sequences(test_sequences,maxlen=max_length,padding='post',truncating='post')","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set up the embedding matrix. We shall use 100 dimensional glove vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {};\nwith open('glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embeddings_matrix.shape)","execution_count":31,"outputs":[{"output_type":"stream","text":"(6703, 100)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding,SpatialDropout1D,LSTM,Bidirectional,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define and compile the model. We shall use an LSTM network with dropouts to prevent overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    SpatialDropout1D(0.2),\n    Bidirectional(LSTM(128,recurrent_dropout=0.2,dropout=0.2)),\n    Dense(32,activation='relu'),\n    Dense(1,activation='sigmoid')\n])\noptimizer = Adam(learning_rate=0.01)\ncallbacks = ReduceLROnPlateau(monitor='val_accuracy',patience=2,factor=0.5,min_lr=0.00001)\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel.summary()","execution_count":33,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 50, 100)           670300    \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 50, 100)           0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 256)               234496    \n_________________________________________________________________\ndense (Dense)                (None, 32)                8224      \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 913,053\nTrainable params: 242,753\nNon-trainable params: 670,300\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 10\nhistory = model.fit(padded_sequences,y_train,epochs=epochs,validation_data=(padded_test_sequences,y_test),batch_size=64,callbacks=[callbacks])","execution_count":35,"outputs":[{"output_type":"stream","text":"Train on 3900 samples, validate on 1672 samples\nEpoch 1/10\n3900/3900 [==============================] - 17s 4ms/sample - loss: 0.2028 - accuracy: 0.9272 - val_loss: 0.0946 - val_accuracy: 0.9707\nEpoch 2/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.1094 - accuracy: 0.9590 - val_loss: 0.0953 - val_accuracy: 0.9701\nEpoch 3/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0803 - accuracy: 0.9710 - val_loss: 0.1061 - val_accuracy: 0.9719\nEpoch 4/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0854 - accuracy: 0.9692 - val_loss: 0.0625 - val_accuracy: 0.9809\nEpoch 5/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0682 - accuracy: 0.9779 - val_loss: 0.0671 - val_accuracy: 0.9809\nEpoch 6/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0478 - accuracy: 0.9854 - val_loss: 0.0668 - val_accuracy: 0.9779\nEpoch 7/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0380 - accuracy: 0.9882 - val_loss: 0.0636 - val_accuracy: 0.9839\nEpoch 8/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0239 - accuracy: 0.9913 - val_loss: 0.0737 - val_accuracy: 0.9797\nEpoch 9/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0279 - accuracy: 0.9908 - val_loss: 0.0750 - val_accuracy: 0.9791\nEpoch 10/10\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.0840 - val_accuracy: 0.9833\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,cohen_kappa_score\ntrain_stats = model.evaluate(padded_sequences,y_train)\ntest_stats = model.evaluate(padded_test_sequences,y_test)\nprint('training accuracy:',train_stats[1]*100)\nprint('testing accuracy:',test_stats[1]*100)\n\ny_pred = model.predict_classes(padded_test_sequences)\nprint(classification_report(y_test,y_pred))\nprint('Confusion matix:\\n',confusion_matrix(y_test,y_pred))\nprint('Cohen-kappa score:',cohen_kappa_score(y_test,y_pred))","execution_count":37,"outputs":[{"output_type":"stream","text":"3900/3900 [==============================] - 2s 570us/sample - loss: 0.0057 - accuracy: 0.9982\n1672/1672 [==============================] - 1s 575us/sample - loss: 0.0840 - accuracy: 0.9833\ntraining accuracy: 99.82051253318787\ntesting accuracy: 98.32535982131958\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      1453\n           1       0.98      0.89      0.93       219\n\n    accuracy                           0.98      1672\n   macro avg       0.98      0.94      0.96      1672\nweighted avg       0.98      0.98      0.98      1672\n\nConfusion matix:\n [[1449    4]\n [  24  195]]\nCohen-kappa score: 0.9234700049367862\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Let us see the effect of using a trainable embedding layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Sequential([\n    Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n    SpatialDropout1D(0.2),\n    Bidirectional(LSTM(128,recurrent_dropout=0.2,dropout=0.2)),\n    Dense(32,activation='relu'),\n    Dense(1,activation='sigmoid')\n])\noptimizer = Adam(learning_rate=0.01)\ncallbacks = ReduceLROnPlateau(monitor='val_accuracy',patience=2,factor=0.5,min_lr=0.00001)\nmodel2.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel2.summary()","execution_count":49,"outputs":[{"output_type":"stream","text":"Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 50, 100)           670300    \n_________________________________________________________________\nspatial_dropout1d_4 (Spatial (None, 50, 100)           0         \n_________________________________________________________________\nbidirectional_4 (Bidirection (None, 256)               234496    \n_________________________________________________________________\ndense_8 (Dense)              (None, 32)                8224      \n_________________________________________________________________\ndense_9 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 913,053\nTrainable params: 913,053\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 15\nimport tensorflow\nclass myCallback(tensorflow.keras.callbacks.Callback):\n    def on_epoch_end(self,epoch,logs={}):\n        if logs['val_accuracy'] > 0.985:\n            self.model.stop_training = True\nmycallback = myCallback()\nhistory = model2.fit(padded_sequences,y_train,epochs=epochs,validation_data=(padded_test_sequences,y_test),batch_size=64,callbacks=[callbacks,mycallback])","execution_count":51,"outputs":[{"output_type":"stream","text":"Train on 3900 samples, validate on 1672 samples\nEpoch 1/15\n3900/3900 [==============================] - 17s 4ms/sample - loss: 0.1924 - accuracy: 0.9505 - val_loss: 0.0657 - val_accuracy: 0.9844\nEpoch 2/15\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0222 - accuracy: 0.9941 - val_loss: 0.0730 - val_accuracy: 0.9791\nEpoch 3/15\n3900/3900 [==============================] - 11s 3ms/sample - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0719 - val_accuracy: 0.9839\nEpoch 4/15\n3900/3900 [==============================] - 12s 3ms/sample - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0708 - val_accuracy: 0.9856\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats2 = model2.evaluate(padded_sequences,y_train)\ntest_stats2 = model2.evaluate(padded_test_sequences,y_test)\nprint('training accuracy:',train_stats2[1]*100)\nprint('testing accuracy:',test_stats2[1]*100)\n\ny_pred2 = model2.predict_classes(padded_test_sequences)\nprint(classification_report(y_test,y_pred2))\nprint('Confusion matix:\\n',confusion_matrix(y_test,y_pred2))\nprint('Cohen-kappa score:',cohen_kappa_score(y_test,y_pred2))","execution_count":52,"outputs":[{"output_type":"stream","text":"3900/3900 [==============================] - 2s 609us/sample - loss: 0.0016 - accuracy: 0.9995\n1672/1672 [==============================] - 1s 623us/sample - loss: 0.0708 - accuracy: 0.9856\ntraining accuracy: 99.94871616363525\ntesting accuracy: 98.56459498405457\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1453\n           1       0.97      0.92      0.94       219\n\n    accuracy                           0.99      1672\n   macro avg       0.98      0.96      0.97      1672\nweighted avg       0.99      0.99      0.99      1672\n\nConfusion matix:\n [[1446    7]\n [  17  202]]\nCohen-kappa score: 0.9356999330207636\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## We obtain better performance using the trainable embedding layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}